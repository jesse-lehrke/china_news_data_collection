{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection from Chinese Science Bulletin - hard scientific articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT CLEANED YET AS OF 21 OCT 2024\n",
    "\n",
    "BUT UPLOADED AS THIS WILL HAPPEN SOON (and uploading creates pressure to do so!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\")\n",
    "options.add_argument(\"--disable-infobars\")\n",
    "#options.add_argument(\"--disable-extensions\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "###################################\n",
    "#options.add_argument(\"--headless\")\n",
    "##### Does not work in headless!!!! #####\n",
    "#prefs = {\"profile.managed_default_content_settings.images\":1} #2\n",
    "#options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "##### This may work in headless #####\n",
    "#options.add_argument('--blink-settings=imagesEnabled=true')\n",
    "####################################\n",
    "\n",
    "options.add_argument(\"user-data-dir=selenium2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(options=options) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.sciengine.com/publisher/scp/journal/CSB/'\n",
    "close = '?slug=browse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume I got these through recon, will have to check and then improve to make this more automatic\n",
    "# See https://www.sciengine.com/CSB/catalogue \n",
    "# could search that to allow year (volumn) with 3 issues per month (1-36, so could also collect by month)\n",
    "# For me that worked, but for most people/use cases not relevant, would want keyword searches\n",
    "\n",
    "volumns =  list(range(62, 68))\n",
    "issues = list(range(1, 37))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = list(itertools.product(volumns, issues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "redo_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vi:\n",
    "    full_url = base_url + str(i[0]) + '/' + str(i[1]) + close\n",
    "    url_list.append(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redo_list = []\n",
    "doi_list = []\n",
    "\n",
    "for url in url_list:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        print(url)\n",
    "        driver.implicitly_wait(10)\n",
    "        content = driver.find_elements(By.PARTIAL_LINK_TEXT,\"doi\")\n",
    "        print('Content found...')\n",
    "\n",
    "        for c in content:\n",
    "            doi_list.append(c.text)\n",
    "        \n",
    "        nap_time = random.uniform(1, 2)\n",
    "        print('Waiting: ' + str(nap_time))\n",
    "        time.sleep(nap_time)\n",
    "        \n",
    "    except:\n",
    "        print('Error')\n",
    "        redo_list.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry = ['https://www.sciengine.com/publisher/scp/journal/CSB/66/28-29?slug=browse', \n",
    "'https://www.sciengine.com/publisher/scp/journal/CSB/66/4-5?slug=browse', \n",
    "'https://www.sciengine.com/publisher/scp/journal/CSB/65/28-29?slug=browse', \n",
    "'https://www.sciengine.com/publisher/scp/journal/CSB/65/2-3?slug=browse', \n",
    "'https://www.sciengine.com/publisher/scp/journal/CSB/64/28-29?slug=browse', \n",
    "'https://www.sciengine.com/publisher/scp/journal/CSB/64/5-6?slug=browse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in retry:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        print(url)\n",
    "        driver.implicitly_wait(10)\n",
    "        content = driver.find_elements(By.PARTIAL_LINK_TEXT,\"doi\")\n",
    "        print('Content found...')\n",
    "\n",
    "        for c in content:\n",
    "            doi_list.append(c.text)\n",
    "        \n",
    "        nap_time = random.uniform(1, 2)\n",
    "        print('Waiting: ' + str(nap_time))\n",
    "        time.sleep(nap_time)\n",
    "        \n",
    "    except:\n",
    "        '''\n",
    "        Did not work due to re-directs.\n",
    "        '''\n",
    "        print('Error')\n",
    "        redo_list.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doi_list, columns=['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "print(len(df))\n",
    "\n",
    "# Gets allot of text info if needed\n",
    "# new = driver.find_elements_by_class_name('dihuias')\n",
    "# for n in new:\n",
    "#     print(n.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[~df['url'].str.contains('csb')]\n",
    "#Alternative\n",
    "#df1 = df[df['url'].str.len() < 41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './DATA/URLS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(PATH + 'csb_urls.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH + 'csb_urls.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#import bs4\n",
    "#import json\n",
    "import pandas as pd\n",
    "import bs4\n",
    "#import csv\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import ssl\n",
    "import urllib.request\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent(verify_ssl=False, cache=False)\n",
    "user_agent = ua.random\n",
    "headers = {'User-Agent': user_agent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(url):\n",
    "    page = urllib.request.Request(url, headers=headers, allow_redirects=False) \n",
    "    time.sleep(5)\n",
    "    uClient = uReq(page)\n",
    "    time.sleep(2)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html, 'lxml')\n",
    "    return page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(page_soup):\n",
    "    time.sleep(1)\n",
    "    result_list = page_soup.find('div', class_= 'article_title')\n",
    "    url_title = result_list.find_all('a') #('a', {'class': 'dihuias'})\n",
    "    dates = result_list.find_all('span')\n",
    "    #urls = page_soup.find_all('li') #('a', {'class': 'dihuias'})\n",
    "    for u in url_title:\n",
    "        item = u['href']\n",
    "        item = base_url + item\n",
    "        url_list.append(item)\n",
    "    for t in url_title:\n",
    "        item = t.text\n",
    "        title_list.append(item)\n",
    "    for d in dates:\n",
    "        item_d = d.text\n",
    "        date_list.append(item_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(index, url):\n",
    "\n",
    "    print('Fetching ' + str(index))\n",
    "    page_soup = make_request(url)        \n",
    "    \n",
    "    nap_time = random.uniform(.2, 1.5)\n",
    "    print('Waiting: ' + str(nap_time))\n",
    "    time.sleep(nap_time)\n",
    "\n",
    "    text = page_soup.find('div', id='content')\n",
    "    text_list.append(text)\n",
    "    #counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "for index, row in df1.iterrows():\n",
    "    try:\n",
    "        connect(index, row.url)\n",
    "    except:\n",
    "        print('Connection error...sleeping for 2 minutes')\n",
    "        time.sleep(120)\n",
    "        print('Retrying...')\n",
    "        connect(index, row.url)\n",
    "print(len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_soup = make_request('https://doi.org/10.1360/N972016-00736')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.head('https://doi.org/10.1360/N972016-00736', allow_redirects=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def expand_url(url):\n",
    "  s = requests.Session()\n",
    "  #s.allow_redirects = -1\n",
    "  try:\n",
    "     r = s.get(url.rstrip(),allow_redirects=3,verify=False)\n",
    "     print([resp.url for resp in r.history])\n",
    "     return r.url.rstrip()\n",
    "  except requests.exceptions.ConnectionError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand_url('https://doi.org/10.1360/N972016-00736')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = page_soup.find_all('//*', id='abstractText')\n",
    "#//*[@id=\"abstractText\"]\n",
    "for t in title:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = page_soup.find('//div', id='fulltextContent')\n",
    "\n",
    "print(title.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = 'https://doi.org/10.1360/N972016-00736'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.url[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(options=options) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#content = driver.find_element(By.XPATH,\"//div[@id='fulltextContent']\")\n",
    "content = get_content()\n",
    "\n",
    "#for c in content:\n",
    "content_list.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = driver.find_element(By.XPATH,\"//div[@id='fulltextContent']\")\n",
    "\n",
    "content2 = content.find_elements(By.CSS_SELECTOR,\"p:not([style='center'])\") #[id='fulltextContent']\") #[content-type='dispformula']\n",
    "for c in content2:\n",
    "    print(c.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = driver.find_element(By.XPATH,\"//div[@id='fulltextContent']\")\n",
    "#aras = content.find_elements(By.XPATH, 'p') \n",
    "\n",
    "table = content.find_elements(By.CSS_SELECTOR,\"p:not([table-wrap])\")\n",
    "for t in table:\n",
    "    print(t.text)\n",
    "#print(content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = []\n",
    "title_list = []\n",
    "date_list = []\n",
    "url_list = []\n",
    "fail_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta():\n",
    "    title = driver.find_element(By.XPATH, \"//*[@id='articletitle']\")\n",
    "    print('Title found...')  \n",
    "    \n",
    "    date = driver.find_element(By.XPATH, \"//ul[@class='xuemaoli']\")\n",
    "    print('Date found...')\n",
    "\n",
    "    return title, date\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content():\n",
    "    try:\n",
    "        #WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, \"//div[@id='sec-001']\")))#.get_attribute(\"value\")\n",
    "        content = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH,\"//div[@id='fulltextContent']\")))\n",
    "        print('Content found...' + str(len(content.text)))\n",
    "        title, text = get_meta()\n",
    "\n",
    "    except:\n",
    "        driver.refresh()\n",
    "        get_content()\n",
    "        \n",
    "    if len(content.text) == 0:\n",
    "        driver.refresh()\n",
    "        driver.implicitly_wait(3)\n",
    "        time.sleep(5)\n",
    "        get_content()\n",
    "        #content = driver.find_element(By.XPATH,\"//div[@id='fulltextContent']\")\n",
    "\n",
    "    return content.text, title, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(index, url):\n",
    "    \n",
    "    \n",
    "    driver.get(url)\n",
    "    print(index)\n",
    "    print(url)\n",
    "    \n",
    "    #driver.implicitly_wait(5)\n",
    "    #driver.set_page_load_timeout(20)\n",
    "    \n",
    "    #content, title, text = get_content()\n",
    "    nap_time = random.uniform(2, 3)\n",
    "    \n",
    "    time.sleep(nap_time)\n",
    "    \n",
    "    try:\n",
    "        #content = WebDriverWait(driver, 30).until(EC.text_to_be_present_in_element_attribute((By.XPATH,\"//div[@id='fulltextContent']\")))\n",
    "        WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.CSS_SELECTOR,\"p\")))\n",
    "        #print('Loaded')\n",
    "        check = driver.find_element(By.CSS_SELECTOR,\"h2\")\n",
    "        print(check.text)\n",
    "        if check.text == 'DOI Not Found':\n",
    "            print('Not found: passing...')\n",
    "            pass\n",
    "        \n",
    "        content = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH,\"//div[@id='fulltextContent']\")))\n",
    "        #content = driver.find_element(By.XPATH,\"//div[@id='fulltextContent']\")\n",
    "        \n",
    "        while len(content.text) == 0 or content.text is None:\n",
    "            print('ZERO')\n",
    "            #driver.refresh()\n",
    "            #driver.implicitly_wait(3)\n",
    "            time.sleep(nap_time)\n",
    "            # time.sleep(5)\n",
    "            #connect(index, url)\n",
    "            content = WebDriverWait(driver, 30).until(EC.text_to_be_present_in_element_attribute((By.XPATH,\"//div[@id='fulltextContent']\")))\n",
    "\n",
    "        #else:\n",
    "        print('Content found...' + str(len(content.text)))\n",
    "\n",
    "        title = driver.find_element(By.XPATH, \"//*[@id='articletitle']\")\n",
    "        print('Title found...')  \n",
    "        \n",
    "        date = driver.find_element(By.XPATH, \"//ul[@class='xuemaoli']\")\n",
    "        print('Date found...')\n",
    "\n",
    "        \n",
    "        #for c in content:\n",
    "        content_list.append(content.text)\n",
    "        #for t in title:\n",
    "        title_list.append(title.text)       \n",
    "        #for d in date:\n",
    "        date_list.append(date.text)       \n",
    "        \n",
    "        url_list.append(url)\n",
    "        \n",
    "        nap_time = random.uniform(4, 6)\n",
    "        print('Waiting: ' + str(nap_time))\n",
    "        time.sleep(nap_time)\n",
    "    \n",
    "    except:\n",
    "        #######NOT YET FUNCTIONAL!\n",
    "        print('Failed')\n",
    "        fail_list.append(url)\n",
    "        pass\n",
    "        #get_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice = df[1008:] # last line -1\n",
    "df_slice.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = 'https://doi.org/10.1360/N972018-01255'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for index, row in df_slice.iterrows():\n",
    "    driver.get(row.url)\n",
    "    driver.implicitly_wait(20)\n",
    "    driver.set_page_load_timeout(20)\n",
    "\n",
    "    url = driver.current_url\n",
    "    print(url)\n",
    "    urls.append(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_slice.iterrows():\n",
    "    try:\n",
    "        connect(index, row.url)\n",
    "        print(str(len(date_list)) + ' - ' + str(len(title_list)) + ' - ' + str(len(content_list)) + ' - ' + str(len(url_list)))\n",
    "    except NoSuchElementException:\n",
    "        print('Failed...')\n",
    "        fail_list.append(row.url)\n",
    "        pass\n",
    "    except:\n",
    "        print('Connection error...sleeping for 2 minutes')\n",
    "        time.sleep(120)\n",
    "        print('Retrying...')\n",
    "        connect(index, row.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(date_list)) + ' - ' + str(len(title_list)) + ' - ' + str(len(content_list)) + ' - ' + str(len(url_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(list(zip(title_list, date_list, content_list, url_list)), columns =['title', 'date', 'text', 'url']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT = './data/batch_data/'\n",
    "df2.to_csv(PATH_OUT + 'csb_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sab23_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd6019e8828702d449dd1edc6815b57e81d872b85877b104edbd3f1715bde4b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
