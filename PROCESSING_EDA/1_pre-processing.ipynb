{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of Chinese text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code prepares scraped texts for NLP tasks and other analysis. Specifically cleaning and tokenizing text, as well as cleaning dates.\n",
    "Columns for some basic counts and other features are also included (and more likely will be added), as well as filtering options (e.g. to drop entries without keywords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BIGGER TO DOES ###########\n",
    "'''\n",
    "1. bi and tri-grams in this script?\n",
    "2. Move all \"counts\" to e.g. feature engineering ??\n",
    "a. I lean against and in fact think all simple feature engineering should get moved here.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "import jieba\n",
    "from jieba import posseg as pseg\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "import time\n",
    "from itertools import combinations, permutations, chain\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess():\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.colnames = ['title', 'url', 'date', 'text']\n",
    "        #self.df = pd.read_csv(self.file, names=self.colnames, header=None, lineterminator='\\n') #  engine='python'\n",
    "        CONVERTERS = {'text': eval}\n",
    "        \n",
    "        self.df = pd.read_csv(self.file, lineterminator='\\n')\n",
    "        self.incomplete_df = None\n",
    "        self.df_complete = None\n",
    "    \n",
    "    def save(self, filename):\n",
    "        filename = filename \n",
    "        \n",
    "        if not os.path.exists('../DATA/processed/'):\n",
    "            os.makedirs('../DATA/processed/') \n",
    "        \n",
    "        self.df_complete.to_csv('../DATA/processed/' + filename + '_clean.csv', index=False)\n",
    "        self.incomplete_df.to_csv('../DATA/processed/' + filename + '_incomplete.csv', index=False)\n",
    "\n",
    "        print('SAVED')\n",
    "   \n",
    "    # pseduo code\n",
    "    def rename_columns(self, column_names=['title', 'url', 'date', 'text']):\n",
    "        new_name_dict = dict(zip(self.colnames, column_names))\n",
    "        self.df.rename(columns=new_name_dict, inplace=True)\n",
    "\n",
    "    def mask_df(self, df, column, mask_keys, inverse=False):\n",
    "        mask = df[column].isin(mask_keys)\n",
    "        if inverse == True:\n",
    "            df = df[~mask]\n",
    "        else:\n",
    "            df = df[mask]\n",
    "        return df\n",
    "    \n",
    "    def get_complete_entries(self):\n",
    "        # This assumes NO english etc in text / if not, just drop 'EMPTY' or whatever you want\n",
    "        self.df.fillna('EMPTY', inplace = True)\n",
    "        #get non-chinese entries from text\n",
    "        mask_keys = list(self.df.text[self.df['text'].str.match(r\"^[a-zA-Z0-9\\s]*$\")].unique())\n",
    "        print('Dropped the following error keys: ' + str(mask_keys))\n",
    "        self.incomplete_df = self.mask_df(self.df, 'text', mask_keys)\n",
    "        self.df_complete = self.mask_df(self.df, 'text', mask_keys, inverse=True)\n",
    "        \n",
    "        # drop empty text strings / BIT REDUNDANT\n",
    "        self.df_complete = self.df_complete[self.df_complete['text'].map(len) > 0] #df[df['TEST'].map(lambda d: len(d)) > 0]\n",
    "        self.incomplete_df_1= self.df_complete[self.df_complete['text'].map(len) < 1]\n",
    "\n",
    "        #drop empty text rows / REDUNDANT - above non-chinese entry check covers this. Why did I keep ?\n",
    "        self.df_complete = self.mask_df(self.df_complete, 'text', ['EMPTY',  'ERROR', '404'], inverse=True)\n",
    "        self.incomplete_df_2= self.mask_df(self.df_complete, 'text', ['EMPTY',  'ERROR', '404'], inverse=False)\n",
    "\n",
    "        self.incomplete_df = pd.concat([self.incomplete_df, self.incomplete_df_1, self.incomplete_df_2])\n",
    "     \n",
    "        print('Complete entries: ' + str(len(self.df_complete)))\n",
    "        print('Problematic entries: ' + str(len(self.incomplete_df)))\n",
    "    \n",
    "    def search_keyword_list(self, keywords):\n",
    "        '''\n",
    "        IN PROGRESSS !!!\n",
    "        '''\n",
    "        def key_check(txt):\n",
    "            keys = []\n",
    "            for k in keywords:\n",
    "                if k in txt:\n",
    "                    keys.append(k)\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            return(keys)\n",
    "        \n",
    "        self.df_complete['keys_present'] = self.df_complete['text'].dropna().apply(lambda x: key_check(x))\n",
    "        \n",
    "        check_list = self.df_complete[self.df_complete['keys_present'].map(len) > 0]\n",
    "        print('At least one keyword in text: ' + str(len(check_list)))\n",
    "\n",
    "    def search_keyword(self, keyword):\n",
    "\n",
    "        keyword = keyword\n",
    "        self.df_complete['in_title'] = self.df_complete['title'].str.contains(keyword)\n",
    "        self.df_complete['in_text'] = self.df_complete['text'].str.contains(keyword)\n",
    "        \n",
    "        # Checking 'search term in' counts\n",
    "        self.df_complete['in_title'].value_counts()\n",
    "        self.df_complete['in_text'].value_counts()\n",
    "        \n",
    "        # Not used: use only if you want to cut anything without key, and without inspection/tokenizing, etc.\n",
    "        # mask = self.df_complete['in_title'] | self.df_complete['in_text'] == True\n",
    "        # df_complete_confirmed = self.df_complete[mask]\n",
    "        \n",
    "        print('Keyword confirmed in title or text: ' + str(len(df_complete_confirmed)))\n",
    "\n",
    "    def filter_keyword_in_tokens(self, keywords, filter=True):\n",
    "        \n",
    "        keep_keys = keywords  # list ['气候']\n",
    "\n",
    "        def keys_only(txt):\n",
    "            return [w for w in txt if w.lower() in keep_keys]\n",
    "\n",
    "        self.df_complete['tokens_keys'] = self.df_complete['hard_tokens'].dropna().apply(lambda x: keys_only(x))\n",
    "\n",
    "        if filter is True:\n",
    "            self.df_complete = self.df_complete[self.df_complete['tokens_keys'].map(lambda d: len(d)) > 0]\n",
    "\n",
    "        print('Original data length: ' + str(len(df)))\n",
    "        print('Dropping: ' + str(len(df_drop)))\n",
    "        print('End data length: ' + str(len(df) - len(df_drop)))\n",
    "\n",
    "\n",
    "    # This can all go for PD, but still has general value - thus perhaps clean up\n",
    "    def datetime_parse(self):\n",
    "        # Removing problems found - goal to make more general, eg. 1 digit follow by 1 letter\n",
    "        self.df_complete['date_standard'] = self.df_complete['date'].str.replace('5G', '')\n",
    "\n",
    "        # Remove all non-alpha-numeric\n",
    "        self.df_complete['date_standard'] = self.df_complete['date_standard'].str.replace(r'[^0-9a-zA-Z:]+', ' ')\n",
    "        # Remove any starting tag with :\n",
    "        self.df_complete['date_standard'] = self.df_complete['date_standard'].str.replace(r'^\\w+:\\s', ' ')\n",
    "        self.df_complete['date_standard'] = self.df_complete['date_standard'].str.split('Updated: ', expand=True)\n",
    "        \n",
    "        # Remove time # TO DO: ensure AM PM optional * but should be since it works!\n",
    "        self.df_complete['date_standard'] = self.df_complete['date_standard'].str.replace(r'\\b(([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9](:[0-5][0-9])?\\s?([AaPp][Mm])?)', ' ')\n",
    "\n",
    "        # TO ADD: check for full month names!\n",
    "        \n",
    "        # Removing digits longer than 4 long and words longer than 3 long\n",
    "        self.df_complete['date_standard'] = self.df_complete['date_standard'].str.replace(r'[1-9]\\d{4,}', ' ')\n",
    "        self.df_complete['date_standard'] = self.df_complete['date_standard'].str.replace(r'[a-zA-Z]\\d{,3}', '')\n",
    "        \n",
    "        # Strip loose whitespace\n",
    "        self.df_complete['date_standard'] = self.df_complete['date_standard'].str.strip()\n",
    "        \n",
    "        # Replacing all non numeric with a standardized entry\n",
    "        self.df_complete = self.df_complete.replace('nan',0)\n",
    "        self.df_complete = self.df_complete.replace('NaN',0)\n",
    "        self.df_complete = self.df_complete.replace('EMPTY',0)\n",
    "        self.df_complete = self.df_complete.replace('',0)\n",
    "        self.df_complete = self.df_complete.fillna(0)\n",
    "\n",
    "                \n",
    "        # Lists for parsing\n",
    "        month = ['%b', '%m', '%B']\n",
    "        day = ['%d']\n",
    "        year = ['%Y', '%y']\n",
    "\n",
    "        varieties = list(permutations(chain(month,day,year), 3))\n",
    "        for v in varieties:\n",
    "            v = ' '.join(v)\n",
    "            try:\n",
    "                self.df_complete['datetime'] = [datetime.strptime(str(d), v) if d != 0 else d for d in self.df_complete['date_standard']]\n",
    "                self.df_complete['datetime'] = [d.date() if d != 0 else d for d in self.df_complete['datetime']]\n",
    "                if self.df_complete['datetime'] is not None:\n",
    "                    print('Successfully parsed with format: ' + v)\n",
    "                    break\n",
    "            except:\n",
    "                #print('Failed: ' + v)\n",
    "                pass\n",
    "        return self.df_complete.head()\n",
    "\n",
    "    \n",
    "    def prep_tokenize(self):\n",
    "        # TO DO: make hard tokenize a param \n",
    "\n",
    "        # Removing loose html code from text\n",
    "        self.df_complete['text'] = self.df_complete['text'].str.replace('\\xa0', '')\n",
    "        self.df_complete['text'] = self.df_complete['text'].str.replace('\\u3000', '') \n",
    "        self.df_complete['text'] = self.df_complete['text'].str.replace('  ', ' ')\n",
    "        self.df_complete['text'] = self.df_complete['text'].str.replace('   ', ' ')\n",
    "        print('HTML tags removed')\n",
    "\n",
    "        def remove_punctuation(txt):\n",
    "            ##############\n",
    "            # UPDATE - check works\n",
    "            ###############\n",
    "\n",
    "            additions = ['：', '，', '《', '。', '》', '“', '„', ':', '一', '・', '«', '»', '”', '“'] \n",
    "            \n",
    "            txt_nopunct = re.sub(r'[^\\w\\s]', '', txt) # removes all non characters, i.e. punct. - suprised it keeps chinese!\n",
    "            txt_nopunct = re.sub(r'\\b\\s+\\b', ' ', txt_nopunct) # removes more than one space\n",
    "\n",
    "            txt_nopunct = txt_nopunct.replace('\\n', '')\n",
    "            txt_nopunct = txt_nopunct.replace('\\t', '')\n",
    "            txt_nopunct = txt_nopunct.replace('\\r', '')\n",
    "            txt_nopunct = txt_nopunct.replace('\\u3000', '') # repetative but to be safe\n",
    "\n",
    "            txt_nopunct = ''.join([c for c in txt_nopunct if c not in additions]) # first regex seems to handle this, but redundancy good\n",
    "\n",
    "            txt_nopunct = txt_nopunct.strip()\n",
    "\n",
    "            return txt_nopunct\n",
    "\n",
    "        def remove_spaces(txt):\n",
    "            txt_nospace = re.sub(r'\\s', '', txt) \n",
    "            return txt_nospace\n",
    "\n",
    "        def tokenize(txt):\n",
    "            #jieba.add_word('于吉', freq=None, tag='nr')\n",
    "            words = jieba.cut(txt, cut_all=False) #HMM=False ## True is default\n",
    "            words = [str(word) for word in words]\n",
    "            return words\n",
    "\n",
    "        def remove_stopwords(txt):\n",
    "            txt_nostop = [w for w in txt if w.lower() not in stop_words]\n",
    "            return txt_nostop\n",
    "        \n",
    "        def hard_tokenize(txt):\n",
    "            regex = re.compile(u'[^\\u4E00-\\u9FA5]')\n",
    "            hard = regex.sub('', txt)\n",
    "            return hard\n",
    "\n",
    "        def get_pos(txt):\n",
    "            #words_pseg = pseg.cut(hard_tokenize(txt)) # use instead of below if text_clean not present\n",
    "            words_pseg = pseg.cut(txt)\n",
    "            pos = [str(x) for x in words_pseg]\n",
    "            return pos\n",
    "\n",
    "        def get_numbers(txt):\n",
    "            x = re.findall(r'\\d+', txt)\n",
    "            return len(x)\n",
    "\n",
    "        def pos_count(pos, txt):\n",
    "            x = [token for token in txt if token.endswith(pos)]\n",
    "            #y = [token.split('/')[0] for token in x] # use when I need lists with just these!\n",
    "            return len(x)\n",
    "            #return y\n",
    "\n",
    "        # Creating initial tokens column just with cleaned text. Unlike normal tokenizing, keeping numbers at this point.\n",
    "\n",
    "        self.df_complete['tokens'] = self.df_complete['text'].dropna().apply(lambda x: remove_punctuation(x))\n",
    "        #self.df_complete['tokens'] = self.df_complete['tokens'].str.lower()\n",
    "        print('Punctuation removed')\n",
    "        \n",
    "        # Features and counts\n",
    "        self.df_complete['numbers_count'] =  self.df_complete['tokens'].apply(lambda x: get_numbers(x))\n",
    "\n",
    "        print('Counts done')        \n",
    "        \n",
    "        # Remove spaces - OPTIONAL ?\n",
    "        #self.df_complete['tokens'] = self.df_complete['tokens'].dropna().apply(lambda x: remove_spaces(x))\n",
    "\n",
    "        # Tokenizing\n",
    "        #self.df_complete['tokens'] = self.df_complete['tokens'].dropna().apply(lambda x: tokenize(x))\n",
    "        #print('Tokenization done')\n",
    "\n",
    "        self.df_complete['text_clean'] = self.df_complete['text'].dropna().apply(lambda x: hard_tokenize(x))\n",
    "        self.df_complete['hard_tokens'] = self.df_complete['text_clean'].dropna().apply(lambda x: tokenize(x))\n",
    "        print('Hard tokenization done')\n",
    "        \n",
    "        # POS list\n",
    "        # WHY I AM USING TEXT CLEAN!!!??? - but it worked... believe it is due to jieba function doing it automatically\n",
    "        self.df_complete['pos_tokens'] = self.df_complete['text_clean'].dropna().apply(lambda x: get_pos(x))\n",
    "        print('POS tagging done')\n",
    "        \n",
    "        self.df_complete['verb_count'] =  self.df_complete['pos_tokens'].apply(lambda x: pos_count('v', x))\n",
    "        self.df_complete['noun_count'] =  self.df_complete['pos_tokens'].apply(lambda x: pos_count('n', x))\n",
    "\n",
    "        print('POS counts done')\n",
    "\n",
    "        # Removing stop words\n",
    "        data = pd.read_csv('../INPUT/stopwords-zh.txt', header=None)\n",
    "        stop_words = data[0].tolist()\n",
    "        \n",
    "        #stop_words_add = ['・', '”', '“', ' ', '\\u3000']\n",
    "        #stop_words = stop_words + stop_words_add\n",
    "\n",
    "        ########## ####\n",
    "        # ATTN - could do to both... but leaving tokens with extra info for now\n",
    "        ###############\n",
    "\n",
    "        #self.df_complete['tokens'] = self.df_complete['tokens'].dropna().apply(lambda txt: remove_stopwords(txt))\n",
    "        self.df_complete['hard_tokens'] = self.df_complete['hard_tokens'].dropna().apply(lambda txt: remove_stopwords(txt))\n",
    "\n",
    "        print('Stop words dropped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start: proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../DATA/'\n",
    "\n",
    "# Getting list of files in data folder\n",
    "csvfiles = [f for f in listdir(PATH) if isfile(join(PATH, f))]\n",
    "print('Index, Filename')\n",
    "print(list(zip([index for index, value in enumerate(csvfiles)], csvfiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file to work with (used index just to keep typing short)\n",
    "index = 4\n",
    "file =  csvfiles[index]\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words and their position of speach tags can be added to jieba like this\n",
    "# useful if you know your text has lots of special terms of importance to your analysis, since tokenizing is not never perfect, more so in Chinese\n",
    "# TO DO: make param for class that allows you to enter a bunch of words (e.g. as a dict) and add them at once.\n",
    "jieba.add_word('极端天气', freq=None, tag='n')\n",
    "jieba.add_word('可持续发展', freq=None, tag='nt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Add a load function so I can use only path and file name in Preprocess\n",
    "# Here just double checking they are what I want\n",
    "print(PATH)\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing = PreProcess(PATH + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a quick check looks ok\n",
    "processing.df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.get_complete_entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search is never perfect: next two functions check our text strings for keywords\n",
    "# If testing using my key and params, this will be shorter the overall length by allot\n",
    "# This is because in 2.1 we collected with broad = True, iirc\n",
    "\n",
    "keywords = ['气候变化', '气候变', '气候变迁', '气候']\n",
    "key_keyword = '气候'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.search_keyword_list(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.search_keyword(key_keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops entries if keyword is not in text tokens, can also set filter=False to keep entries but still create column saying which keywords are in text\n",
    "# filter_keyword_in_tokens(self, keywords, filter=True):\n",
    "# TO DO: test!!! as I moved this from elsewhere and could have made an error\n",
    "\n",
    "processing.filter_keyword_in_tokens(['气候'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If date gotten elsewhere, e.g. for People Daily, not needed, but still nice to run: can confirm day, month order etc.\n",
    "processing.datetime_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our cleaning and tokenizer\n",
    "\n",
    "processing.prep_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a last look at df\n",
    "\n",
    "processing.df_complete.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Move into Class\n",
    "# OR KEEP? Has numbers... figure out what to do with numbers, then decide\n",
    "# best: turn numbers into words...\n",
    "processing.df_complete.drop('tokens', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Move into class\n",
    "filename = file.split('.')[0]\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case you need it, placed here for convienence for now. But eventually will be moved or merged elsewhere... maybe a check language function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep in mind you need to install nltk stuff iirc\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(txt):\n",
    "    txt_nostop = [w for w in txt if w.lower() not in stop_words]\n",
    "    return txt_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.dropna().apply(lambda row: nltk.word_tokenize(row['tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].dropna().apply(lambda txt: remove_stopwords(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is stuff I've cut. Place here for now in case latter in process I realize I need it, but will evenutally be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def datetime_from_clean(self):\n",
    "        # If you know your dates are pretty clean can use this instead\n",
    "        \n",
    "        # Lists for parsing\n",
    "        month = ['%b', '%m', '%B']\n",
    "        day = ['%d']\n",
    "        year = ['%Y', '%y']\n",
    "        \n",
    "        self.df_complete['date_standard'] = self.df_complete['date_standard'].str.strip()\n",
    "        self.df_complete['date_standard'] = self.df_complete['date_standard'].replace('0', 0)\n",
    "\n",
    "        varieties = list(permutations(chain(month,day,year), 3))\n",
    "        for v in varieties:\n",
    "            v = ' '.join(v)\n",
    "            try:\n",
    "                self.df_complete['datetime'] = [datetime.strptime(str(d), v) if d != 0 else d for d in self.df_complete['date_standard']]\n",
    "                self.df_complete['datetime'] = [d.date() if d != 0 else d for d in self.df_complete['datetime']]\n",
    "                if self.df_complete['datetime'] is not None:\n",
    "                    print('Successfully parsed with format: ' + v)\n",
    "                    break\n",
    "            except:\n",
    "                print('Failed: ' + v)\n",
    "                pass\n",
    "        return self.df_complete.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
