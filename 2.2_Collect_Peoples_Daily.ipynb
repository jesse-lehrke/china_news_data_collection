{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting URLs, via e.g. 2.1_Requests_Peoples_Daily, this script collects the text content.\n",
    "\n",
    "It uses multi-threading so runs much faster than other approaches. \n",
    "\n",
    "It also contains some very basic preprocessing, two functions, which are essential at a foundational level. Still, considering moving those.\n",
    "\n",
    "NEXT STEP: make into a py file as well for future seemless running of requests for url and this for text collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd\n",
    "import codecs\n",
    "\n",
    "import concurrent.futures # for multi-threading\n",
    "import time\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from urllib3.exceptions import ReadTimeoutError\n",
    "from requests.exceptions import ProxyError, HTTPError, ConnectionError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH =  './DATA/'\n",
    "INPUT_PATH = './INPUT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output path folder if not exists (no need to do for input, must exist, create that elsewhere)\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function, I used toffigure out which csv to use for URL_FILE if I have lots\n",
    "csvfiles = [f for f in listdir(INPUT_PATH) if isfile(join(INPUT_PATH, f))]\n",
    "print('Index, Filename')\n",
    "print(list(zip([index for index, value in enumerate(csvfiles)], csvfiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_FILE = 'peoples_request_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"peoples_daily_collected.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty df with column names for continuous saving during collection\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH + OUTPUT_FILE):\n",
    "    df = pd.DataFrame(columns=['title','url', 'date','text'])\n",
    "    df.to_csv(OUTPUT_PATH + OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main collection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_scrape(url, title):\n",
    "    main = 'http://www.people.com.cn/'\n",
    "    print('Scraping ' + url)\n",
    "\n",
    "    ua = UserAgent()\n",
    "    user_agent = ua.random\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    try:\n",
    "        with requests.get(url, headers= headers, timeout=10) as response:\n",
    "           page_soup = soup(response.content, 'lxml', from_encoding=response.encoding) #\"iso-8859-1\"\n",
    "\n",
    "    except HTTPError:\n",
    "        print('HTTPError. Passing...')\n",
    "        body_no+=1\n",
    "        body = 'HTTPError'\n",
    "        date = 'HTTPError'\n",
    "        \n",
    "        with codecs.open(OUTPUT_PATH + OUTPUT_FILE, 'a', 'utf-8',) as f:\n",
    "            f.write(title + ',' + url + ',' + date + ',' + body + '\\n')\n",
    "        \n",
    "        pass            \n",
    "    \n",
    "    except ConnectionError as exc:\n",
    "        if 'Errno 113' in str(exc):\n",
    "            print('Dead link. Passing...')\n",
    "            body_no+=1\n",
    "            body = 'DEAD'\n",
    "            date = 'DEAD'\n",
    "                            \n",
    "            with codecs.open(OUTPUT_PATH + OUTPUT_FILE, 'a', 'utf-8',) as f:\n",
    "                f.write(title + ',' + url + ',' + date + ',' + body + '\\n')\n",
    "            \n",
    "            pass\n",
    "\n",
    "        if 'Errno -3' in str(exc):\n",
    "            print('Connection error...sleeping for 2 minutes')\n",
    "            connection_retry()\n",
    "\n",
    "    except ReadTimeoutError:\n",
    "            print('TIMEOUT. Passing...')\n",
    "            body_no+=1\n",
    "            body = 'TIMEOUT'\n",
    "            date = 'TIMEOUT'\n",
    "                            \n",
    "            with codecs.open(OUTPUT_PATH + OUTPUT_FILE, 'a', 'utf-8',) as f:\n",
    "                f.write(title + ',' + url + ',' + date + ',' + body + '\\n')\n",
    "            \n",
    "    except requests.exceptions.Timeout: \n",
    "            print('TIMEOUT. Passing...')\n",
    "            body_no+=1\n",
    "            body = 'TIMEOUT'\n",
    "            date = 'TIMEOUT'\n",
    "                            \n",
    "            with codecs.open(OUTPUT_PATH + OUTPUT_FILE, 'a', 'utf-8',) as f:\n",
    "                f.write(title + ',' + url + ',' + date + ',' + body + '\\n')\n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    if response is None:\n",
    "        print('Empty response..skipping')\n",
    "\n",
    "    else:\n",
    "        print('Response good from ' + response.url)\n",
    "\n",
    "    if response.url == 'http://www.people.cn/404/error.html' or response.url == main:\n",
    "        print('Page deleted by website owner. Passing...')\n",
    "        body = '404'\n",
    "        date = '404'\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            classes = ['box_con', 'text', 'show_text', 'rm_txt_con cf', 'content', 'rwb_zw', 'rm_txt_con cf',\n",
    "                        'content clear clearfix']\n",
    "\n",
    "            # Text collection\n",
    "            for c in classes:\n",
    "                body = page_soup.find('div', class_=c)\n",
    "                try:\n",
    "                    body = body.text\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    body = body.replace(\",\",\"\").strip()\n",
    "                    body = body.replace(\"\\n\", \" \")\n",
    "                except:\n",
    "                    body = 'EMPTY'\n",
    "\n",
    "            # Title collection not need as it is in original request data; left here just in case needed for some edge case\n",
    "            # try:\n",
    "            #     title = page_soup.find_all('h1')\n",
    "\n",
    "            #     if len(title) == 1:\n",
    "            #         title = title[0].text\n",
    "            #     else:\n",
    "            #         title = [t.text for t in title]\n",
    "            #         title = max(title, key=len)\n",
    "\n",
    "            # except:\n",
    "            #     title = page_soup.find('h2')\n",
    "            #     try:\n",
    "            #         title = title.text \n",
    "            #     except:\n",
    "            #         title = 'EMPTY'\n",
    "            \n",
    "            # Date collection deleted, this can be gotten from url much easier\n",
    "            # Per my practice, to be done AFTER scrape\n",
    "        \n",
    "        except:\n",
    "            print('Whoops ... FAIL') # yeah, unlikely to ever hit this, which is why its so casual\n",
    "    \n",
    "\n",
    "    with codecs.open(OUTPUT_PATH + OUTPUT_FILE, 'a') as f: # previously also used response.encoding parameter, gives issues if using row.title though\n",
    "        # Keep this\n",
    "        title = title.strip()\n",
    "        title = title.replace(\",\",\"\").strip()\n",
    "        title = title.replace(\"\\n\", \" \")\n",
    "\n",
    "        body = body.strip()\n",
    "        body = body.replace(\",\",\" \").strip()\n",
    "        body = body.replace(\"\\n\", \" \")\n",
    "\n",
    "        # Just a filler per the above; may remove all later\n",
    "        date = 'PLACEHOLDER'\n",
    "        \n",
    "        f.write(title + ',' + url + ',' + date + ',' + body + '\\n')\n",
    "\n",
    "    print('Thread done')\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing\n",
    "# for index, row in df.iterrows():\n",
    "#     sub_scrape(row.url, row.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load url data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_PATH + URL_FILE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need all the columns\n",
    "# Some could have value for your research though, so now would be the point to check and think whether to include\n",
    "# TO DO: add subtitle, merge with title column though\n",
    "\n",
    "df = df[['title', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing\n",
    "#df = df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(df, size=10):\n",
    "      size = size\n",
    "      \n",
    "      l = list(range(size, len(df), size))\n",
    "      l_mod = [0] + l + [max(l) + size]\n",
    "      list_of_dfs = [df.iloc[l_mod[n]:l_mod[n+1]] for n in range(len(l_mod)-1)]\n",
    "            \n",
    "      return list_of_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = batch(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1 \n",
    "\n",
    "for df in list_of_dfs:\n",
    "\n",
    "      no_threads = 10\n",
    "\n",
    "\n",
    "      with concurrent.futures.ThreadPoolExecutor(max_workers=no_threads) as executor:\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                  print(f\"Thread starting for: {row.url}\")\n",
    "                  \n",
    "                  executor.submit(sub_scrape, row.url, row.title)\n",
    "      \n",
    "      print('Batch done: ' + str(counter))\n",
    "\n",
    "      counter +=1 \n",
    "\n",
    "      time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing - Maybe move to other script, but this is pretty basic and essential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(OUTPUT_PATH + OUTPUT_FILE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peoples Daily has the date in url - this extracts it, saving a step elsewhere\n",
    "# Note: range has to be edited as needed! Recall top range needs to be +1 \n",
    "\n",
    "def date_from_url(url):\n",
    "      years = list(range(2000, 2025, 1))\n",
    "      for year in years:\n",
    "            year = str(year)\n",
    "            split_on ='/' + year + '/'\n",
    "\n",
    "            if split_on in url:\n",
    "                  date_start = url.split(split_on)[-1]\n",
    "                  date_mid = date_start.split('/')[0]\n",
    "                  month = date_mid[:2]\n",
    "                  day = date_mid[2:]\n",
    "                  date = month + '-' +  day + '-' + year\n",
    "\n",
    "                  # failsafe, but do not recall why needed - I trust my old self though, will try to check/follow-up\n",
    "                  if len(date) != 10:\n",
    "                        date = '01-01-' + year\n",
    "                  else:\n",
    "                        return date\n",
    "            else:\n",
    "                  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['url'].apply(lambda url: date_from_url(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    try:\n",
    "        text = x.encode('iso-8859-1').decode('EUC-CN', 'ignore')\n",
    "        return text\n",
    "    except:\n",
    "        return x\n",
    "        print(x)\n",
    "    \n",
    "    # Legacy: will delete later after I recall why I checked or used this at one time\n",
    "    #body = body.decode(response.encoding, \"strict\").encode(\"utf8\", \"strict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply convert function, title's are fine unless you maybe recollect\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: convert(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaving this final check in here\n",
    "\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwriting file here! But this is a rare exception: original file without data and encoding is useless\n",
    "\n",
    "CLEANED_OUTPUT = \"peoples_daily_collected.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_PATH + CLEANED_OUTPUT, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collect_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
