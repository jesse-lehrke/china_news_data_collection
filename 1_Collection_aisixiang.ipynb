{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection from aisixiang.com - political/philosophical scientific articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONLY PARTIALLY CLEANED AS OF 21 OCT \n",
    "BUT UPLOADED AS THIS WILL HAPPEN SOON (and uploading creates pressure to do so!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "from urllib.request import urlopen as uReq\n",
    "import urllib.request\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "import ssl\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get collection URLS etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent(verify_ssl=False) #cache=False # seems this option was removed\n",
    "user_agent = ua.random\n",
    "headers = {'User-Agent': user_agent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.aisixiang.com'\n",
    "search_url = \"/data/search.php?lanmu=\" #n\n",
    "pagination = '&page=' #n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may require some website recon, maybe make a dict, to make more automatic for end users\n",
    "lanmu_list = ['677', '678', '676', '682', '683', '680', '681', '684', '679', '685', '688', '703', '482'] # econ, socio[1], peda[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(url):\n",
    "    page = urllib.request.Request(url, headers=headers) \n",
    "    uClient = uReq(page)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html, 'lxml')\n",
    "    return page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(page_soup):\n",
    "    time.sleep(1)\n",
    "    result_list = page_soup.find('div', class_= 'search_list')\n",
    "    url_title = result_list.find_all('a') #('a', {'class': 'dihuias'})\n",
    "    dates = result_list.find_all('span')\n",
    "    #urls = page_soup.find_all('li') #('a', {'class': 'dihuias'})\n",
    "    for u in url_title:\n",
    "        item = u['href']\n",
    "        item = base_url + item\n",
    "        url_list.append(item)\n",
    "    for t in url_title:\n",
    "        item = t.text\n",
    "        title_list.append(item)\n",
    "    for d in dates:\n",
    "        item_d = d.text\n",
    "        date_list.append(item_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "title_list = []\n",
    "date_list = []\n",
    "\n",
    "counter = 1\n",
    "#max_page = 2\n",
    "for lanmu in lanmu_list:\n",
    "    print('Searching: ' + lanmu)\n",
    "    initial_page = base_url + search_url + lanmu + pagination + str(counter)\n",
    "    page_soup = make_request(initial_page)\n",
    "\n",
    "    #max page\n",
    "    page_list = page_soup.find('div', class_= 'list_page')\n",
    "    n_pages = page_list.find_all('a') \n",
    "    max_page = int(n_pages[-1]['href'].split('=')[-1])\n",
    "    print('Pages: ' + str(max_page))\n",
    "\n",
    "    while counter <= max_page:\n",
    "        search_page = base_url + search_url + lanmu + pagination + str(counter)\n",
    "        page_soup = make_request(search_page)\n",
    "        print('Searching ' + search_page)\n",
    "        \n",
    "        get_details(page_soup)\n",
    "\n",
    "        if counter == max_page:\n",
    "            counter = 0\n",
    "            print('Next catagory ... ')\n",
    "            break\n",
    "        \n",
    "        time.sleep(1)\n",
    "        counter +=1\n",
    "\n",
    "url_list_filter = [u for u in url_list if 'lanmu' not in u]\n",
    "title_list_filter = [u for u in title_list if '[理论法学]' not in u]\n",
    "\n",
    "df = pd.DataFrame(list(zip(url_list_filter, date_list, title_list_filter)), columns =['url', 'date', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './DATA/URLS/'\n",
    "\n",
    "df.to_csv(PATH + 'aisixiang_urls.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "df = pd.read_csv(PATH + 'aisixiang_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['date'] >= '2017-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Full Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(index, url):\n",
    "\n",
    "    print('Fetching ' + str(index))\n",
    "    page_soup = make_request(url)        \n",
    "    \n",
    "    nap_time = random.uniform(.2, 1.5)\n",
    "    print('Waiting: ' + str(nap_time))\n",
    "    time.sleep(nap_time)\n",
    "\n",
    "    text = page_soup.find('div', id='content')\n",
    "    text_list.append(text.text) \n",
    "    #counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        connect(index, row.url)\n",
    "    except:\n",
    "        print('Connection error...sleeping for 2 minutes')\n",
    "        time.sleep(120)\n",
    "        print('Retrying...')\n",
    "        connect(index, row.url)\n",
    "print(len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = text_list #pd.DataFrame(url_list, text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = './DATA/'\n",
    "\n",
    "df.to_csv(OUT_PATH + 'aisixiang_full.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sab23_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd6019e8828702d449dd1edc6815b57e81d872b85877b104edbd3f1715bde4b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
